//! Single worker loop: dequeue requests and execute inference.
//!
//! All inference goes through this worker. The IPC handler enqueues
//! requests and awaits the oneshot response channel.

use std::sync::Arc;
use tokio::task::JoinHandle;

use crate::engine::InferenceEngine;
use crate::memory::ResourceLimits;
use crate::models::lifecycle::ModelLifecycle;
use crate::models::registry::ModelRegistry;
use crate::telemetry;
use super::queue::{QueuedRequest, RequestQueue};

/// Spawn the worker loop. Returns a handle for shutdown.
pub fn spawn_worker(
    queue: Arc<RequestQueue>,
    engine: Arc<InferenceEngine>,
    shutdown: tokio_util::sync::CancellationToken,
) -> JoinHandle<()> {
    spawn_worker_with_registry(queue, engine, None, None, None, shutdown)
}

/// Spawn with optional registry for per-model stats recording and resource limits.
pub fn spawn_worker_with_registry(
    queue: Arc<RequestQueue>,
    engine: Arc<InferenceEngine>,
    lifecycle: Option<Arc<ModelLifecycle>>,
    registry: Option<Arc<ModelRegistry>>,
    resource_limits: Option<ResourceLimits>,
    shutdown: tokio_util::sync::CancellationToken,
) -> JoinHandle<()> {
    tokio::spawn(async move {
        worker_loop(
            &queue,
            &engine,
            lifecycle.as_deref(),
            registry.as_deref(),
            resource_limits.as_ref(),
            shutdown,
        )
        .await;
    })
}

async fn worker_loop(
    queue: &RequestQueue,
    engine: &InferenceEngine,
    lifecycle: Option<&ModelLifecycle>,
    registry: Option<&ModelRegistry>,
    resource_limits: Option<&ResourceLimits>,
    shutdown: tokio_util::sync::CancellationToken,
) {
    loop {
        tokio::select! {
            biased;
            () = shutdown.cancelled() => {
                tracing::info!("worker: shutdown signal received");
                break;
            }
            req_opt = queue.wait_and_dequeue() => {
                if let Some(request) = req_opt {
                    execute_request(engine, lifecycle, registry, resource_limits, request).await;
                }
            }
        }
    }
}

async fn execute_request(
    engine: &InferenceEngine,
    lifecycle: Option<&ModelLifecycle>,
    registry: Option<&ModelRegistry>,
    resource_limits: Option<&ResourceLimits>,
    request: QueuedRequest,
) {
    let model_id = request.model_id.clone();
    let prompt = request.prompt.clone();
    let params = request.params.clone();
    let cancelled = request.cancel_check();

    // Admission control: acquire resource slot before inference.
    let _guard = if let Some(limits) = resource_limits {
        let memory_estimate = estimate_memory(engine, &model_id).await;
        match limits.try_acquire(memory_estimate) {
            Ok(guard) => Some(guard),
            Err(e) => {
                telemetry::record_admission_rejection(&model_id, &e.to_string());
                let mapped: Result<crate::engine::inference::InferenceResult, String> =
                    Err(e.to_string());
                send_response(request, mapped);
                return;
            }
        }
    } else {
        None
    };

    let start = std::time::Instant::now();

    // Run inference â€” if resource limits are active, pass the per-call memory
    // budget so the engine can enforce it coherently with InferenceConfig.
    let result = if let Some(limits) = resource_limits {
        engine
            .run_cancellable_with_memory_limit(
                &model_id, &prompt, &params, cancelled,
                limits.max_memory_per_call(),
            )
            .await
    } else {
        engine
            .run_cancellable(&model_id, &prompt, &params, cancelled)
            .await
    };
    let latency_ms = start.elapsed().as_millis() as u64;

    match &result {
        Ok(r) => {
            telemetry::record_request_success(
                &model_id, latency_ms, r.tokens_generated as u64,
            );
            // Record per-model stats in ModelRegistry if available
            if let (Some(lc), Some(reg)) = (lifecycle, registry) {
                if let Some(handle) = lc.get_handle(&model_id).await {
                    reg.record_request(handle, latency_ms as f64).await;
                }
            }
        }
        Err(e) => {
            telemetry::record_request_failure(&model_id, &e.to_string());
        }
    }

    let mapped = result.map_err(|e| e.to_string());
    send_response(request, mapped);
    // _guard drops here, releasing the resource slot.
}

/// Estimate memory for a model. Falls back to a fixed 256 MiB if not registered.
async fn estimate_memory(engine: &InferenceEngine, model_id: &str) -> usize {
    const FALLBACK_BYTES: usize = 256 * 1024 * 1024;
    engine
        .model_memory_usage(model_id)
        .await
        .unwrap_or(FALLBACK_BYTES)
}

fn send_response(request: QueuedRequest, result: Result<crate::engine::inference::InferenceResult, String>) {
    if let Some(tx) = request.response_tx {
        let _ = tx.send(result);
    }
}

#[cfg(test)]
#[path = "worker_tests.rs"]
mod tests;
